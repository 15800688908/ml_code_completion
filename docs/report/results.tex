%\vspace{-5pt}
\section{Experiments and Results}
\label{sec:results}

In this section, we evaluate the methods outlined in Section~\ref{sec:methods},
on Linux kernel and Twisted library. We first present the accuracy of
predictions on the test set, and then present some interesting predictions that
come out from recurring patterns. We implemented the matrix vector model in
Python, and the feed-forward and recurrent models in Keras~\cite{ref:keras}.
Our code is available on github~\cite{ref:codecompletion}.

\subsection{Accuracy}
\label{sec:accuracy}

\begin{table}[h]
  \centering
  \small {
  \begin{tabular}{l l l r r r r r}
    \hline
    Method & Known & Abs & Top 3 & Key & Pos \\
    Win, \#Keys & acc. & acc. & acc.  & acc. & acc. \\
    \hline
    Dense-3, 40, 2000 & 64.5 & x & 81.8 & 78.2 & 43.6\\
    Dense-4, 40, 2000 & 63.2 & x & 82.0 & 72.8 & 41.8\\
    Attn, 40, 1000 & 67.6 & x & 83.6 & 80.0 & 48.4\\
    GRU, 40, 2000 & x & x & x & x & x\\
    Random & x & x & x & x & x\\
    \hline
  \end{tabular}
  }
  \caption{Test accuracy (\%) of predictions for Linux project}
  \label{tab:linux}
\end{table}

\begin{table}[h]
  \centering
  \small {
  \begin{tabular}{l l l r r r r r}
    \hline
    Method & Known & Abs & Top 3 & Key & Pos \\
    Win, \#Keys & acc. & acc. & acc.  & acc. & acc. \\
    \hline
    Dense-3, 40, 500 & 46.3 & x & 64.8 & 64.0 & 14.0\\
    Dense-4, 40, 500 & 43.0 & 38.7 & 59.0 & 65.5 & 9.2\\
    Attn, 40, 500 & 46.6 & x & 64.7 & 62.5 & 18.5\\
    GRU, 40, 500 & 41.6 & x & 59.1 & 64.4 & 4.9\\
    Random & x & x & x & x & x\\
    \hline
  \end{tabular}
  }
  \caption{Test accuracy (\%) of predictions for Twisted project}
  \label{tab:twisted}
\end{table}

\noindent
Table~\ref{tab:linux} shows results for Linux source, a C project, and
Table~\ref{tab:twisted} for Twisted, a Python networking library. As mentioned
in Section~\ref{sec:dataset}, we use half the files for training and half for
testing.
Column 1
lists out the learning method. We also report numbers for a naive, random
predictor, that predicts a random token as the next token. Known acc. is the
accuracy for predictions for the cases where the next token is a key token, or
a positional token from the window being considered. Abs acc. is the accuracy
for predictions for all cases, considering the cases where the next token is
neither a key word nor a seen positional token. These are often new function
names or variable names, or variables outside of our window. Top 3 is the
percentage of cases where the next token is within top 3 predictions, ignoring
the unknown cases (unseen variable and function names). Key acc. is the
accuracy with which we predict key tokens correctly, given the next token is a
key token,
and pos. acc. is the accuracy with which we predict positional tokens
correctly, given the next token is a positional token.

We observed that among the models discussed, the Non-Linear Matrix Vector Model
gave the best accuracy (55\% accuracy as opposed to $\approx$ 45\% for Fixed
Window Weight Model and $\approx$ 50\% for Matrix Vector Model). Overall, we
found that 46\% of the windows had an output that is a key token, and 30\% had
an output that was a positional token. This underlines the importance of also
considering non-keyword tokens while doing prediction.

The accuracy of predicting the keyword tokens was {\bf 72\%}, while the accuracy
of predicting the non-keyword tokens was {\bf 29\%}. We also evaluated the
accuracy of a random predictor on the non-keyword tokens, and that achieves an
accuracy of 8\%, which is significantly lower than the accuracy achieved by our
model (29\%). Overall, the accuracy on windows with keyword/positional tokens is
{\bf 55\%}, which translates to an accuracy of 42\% over all windows (including
windows with UNKNOWN output, which if outside of the capability of our model to
predict).

